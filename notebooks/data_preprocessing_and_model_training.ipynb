{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building AI model to classify logs into P1, P2, P3 and P4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Efficient Data Loading\n",
    "- ***Chunk Loading:*** Due to dataset size, data is loaded in chunks to prevent memory overflow.\n",
    "- ***PySpark Integration:*** For scalability, Pyspark is utilized for parallel data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rshekar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/rshekar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hw/2qqt9k2547j0gf2k4f_hq0lc0000gn/T/ipykernel_47201/3486445007.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  logs_df = pd.concat(chunks, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500000\n",
    "chunks = pd.read_csv('../datasets/processed_logs.csv', chunksize=chunk_size)\n",
    "logs_df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8505002 entries, 0 to 8505001\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype \n",
      "---  ------     ----- \n",
      " 0   timestamp  object\n",
      " 1   level      object\n",
      " 2   message    object\n",
      " 3   source     object\n",
      "dtypes: object(4)\n",
      "memory usage: 259.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(logs_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Data Inspection\n",
    "- ***Schema:*** The dataset consists of timestamp, level, message and source columns.\n",
    "- ***Null Values:*** Checked for missing or malformed entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp   level  \\\n",
      "0  Thu Jun 09 06:07:04 2005  notice   \n",
      "1  Thu Jun 09 06:07:04 2005  notice   \n",
      "2  Thu Jun 09 06:07:04 2005  notice   \n",
      "3  Thu Jun 09 06:07:05 2005  notice   \n",
      "4  Thu Jun 09 06:07:05 2005  notice   \n",
      "\n",
      "                                             message  source  \n",
      "0                 LDAP: Built with OpenLDAP LDAP SDK  Apache  \n",
      "1                      LDAP: SSL support unavailable  Apache  \n",
      "2  suEXEC mechanism enabled (wrapper: /usr/sbin/s...  Apache  \n",
      "3  Digest: generating secret for digest authentic...  Apache  \n",
      "4                                       Digest: done  Apache  \n",
      "timestamp    2500000\n",
      "level              0\n",
      "message          101\n",
      "source             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(logs_df.head())\n",
    "\n",
    "print(logs_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA) and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level\n",
      "INFO                                5954567\n",
      "WARN                                 833297\n",
      "ERROR                                625355\n",
      "CRITICAL                             624647\n",
      "FATAL                                321471\n",
      "                                     ...   \n",
      "GoogleSoftwareUpdateAgent[35089]          1\n",
      "netbiosd[35901]                           1\n",
      "netbiosd[31279]                           1\n",
      "helpd[36107]                              1\n",
      "GoogleSoftwareUpdateAgent[33940]          1\n",
      "Name: count, Length: 1040, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(logs_df['level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The level column has 1,040 unique values, but standard log levels like INFO, WARN, ERROR, CRITICAL, and FATAL dominate the dataset. The remaining values appear to be process names or non-standard log levels, such as GoogleSoftwareUpdateAgent[35089], netbiosd[35901], etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating standard log levels and process names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_level\n",
      "INFO        5954567\n",
      "WARN         833465\n",
      "ERROR        663436\n",
      "CRITICAL     624647\n",
      "FATAL        321471\n",
      "OTHER         93661\n",
      "NOTICE        13755\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "standard_levels = ['INFO', 'WARN', 'ERROR', 'CRITICAL', 'FATAL', 'NOTICE', 'DEBUG']\n",
    "\n",
    "logs_df[\"cleaned_level\"] = logs_df[\"level\"].apply(lambda x: x.upper() if x.upper() in standard_levels else 'OTHER')\n",
    "\n",
    "print(logs_df[\"cleaned_level\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting log source (process names) if 'level' is not standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df[\"log_source\"] = logs_df.apply(lambda row: row[\"level\"] if row[\"cleaned_level\"] == \"OTHER\" else \"SYSTEM\", axis=1)\n",
    "\n",
    "print(logs_df[\"log_source\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Preprocessing (NLP)\n",
    "- ***Lowercasing:*** Coverting all messages to lowercase.\n",
    "- ***Punctuation Removal:*** Removed unnecessary punctuation.\n",
    "- ***Stopword Removal:*** Eliminated common stopwords using NLTK.\n",
    "- ***Lemmatization:*** Reduced words to their base forms for uniformity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean log messages\n",
    "def preprocess_log_message(message):\n",
    "    # Checking if the message is str object if not replacing NaN or non-string values with an empty string\n",
    "    if isinstance(message, str):\n",
    "        message = ''\n",
    "    message = message.lower()\n",
    "    message = re.sub(r'[^a-zA-Z0-9\\s]', '', message)\n",
    "    message = ' '.join([lemmatizer.lemmatize(word) for word in message.split() if word not in stop_words])\n",
    "    return message\n",
    "\n",
    "# logs_df['message'] = logs_df['message'].fillna('No message provided')\n",
    "# logs_df['cleaned_message'] = logs_df[\"message\"].apply(preprocess_log_message)\n",
    "# print(logs_df[\"message\"].head(10))\n",
    "print(logs_df['message'].isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alertops-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
