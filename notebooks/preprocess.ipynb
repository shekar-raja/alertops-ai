{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis for Loghub Datasets: https://github.com/logpai/loghub?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets variable with paths to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Apache\": \"../datasets/Apache.log\",\n",
    "    \"BGL\": \"../datasets/BGL/BGL.log\",\n",
    "    \"HDFS\": \"../datasets/HDFS_V1/HDFS.log\",\n",
    "    \"Linux\": \"../datasets/Linux.log\",\n",
    "    \"Mac\": \"../datasets/Mac.log\",\n",
    "    \"OpenStack_Normal1\": \"../datasets/OpenStack/openstack_normal1.log\",\n",
    "    \"OpenStack_Normal2\": \"../datasets/OpenStack/openstack_normal2.log\",\n",
    "    \"OpenStack_Abnormal\": \"../datasets/OpenStack/openstack_abnormal.log\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function extracts three parts from the log entry:\n",
    "1. Log level\n",
    "2. Message\n",
    "3. Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(file_path, source_name):\n",
    "    logs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        for i, log_line in enumerate(file):\n",
    "            if i >= 2000000: # Limiting logs to 10000,000\n",
    "                break\n",
    "\n",
    "            match = None\n",
    "            log_pattern = None\n",
    "            try:\n",
    "                if (source_name == \"Apache\"):\n",
    "                    log_pattern = r\"\\[(.*?)\\] \\[(.*?)\\] (.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"BGL\"):\n",
    "                    log_pattern = r\"- \\d+ \\d{4}\\.\\d{2}\\.\\d{2} ([\\w:-]+) [\\d\\-:.]+ [\\w:-]+ ([\\w\\s]+) (.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"HDFS\"):\n",
    "                    log_pattern = r\"\\d{6} \\d{6} \\d+ (INFO|WARN|ERROR) ([\\w\\.$]+): (.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(1),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"Linux\"):\n",
    "                    log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+([\\w-]+)\\s+([\\w.$]+):\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(3),\n",
    "                            \"message\": match.group(4),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"Mac\"):\n",
    "                    log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+([\\w-]+)\\s+([\\w\\[\\]0-9]+):\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(3),\n",
    "                            \"message\": match.group(4),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"OpenStack_Normal1\" or \"OpenStack_Normal2\" or \"OpenStack_Abnormal\"):\n",
    "                    log_pattern = r\"^\\S+\\s+\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\\.\\d+\\s+\\d+\\s+(INFO|ERROR|WARN|DEBUG)\\s+([\\w\\.\\[\\]]+)\\s+(?:\\[.*?\\])?\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(1),\n",
    "                            \"message\": match.group(2),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "                    \n",
    "    return pd.DataFrame(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_file(file_path, source_name):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"source\"] = source_name  # Add a source column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing log file: ../datasets/Apache.log\n",
      "Parsing log file: ../datasets/BGL/BGL.log\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing log file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mparse_log_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     merged_logs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m, in \u001b[0;36mparse_log_file\u001b[0;34m(file_path, source_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (source_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGL\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     21\u001b[0m     log_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+ \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m ([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw:-]+) [\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-:.]+ [\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw:-]+ ([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]+) (.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_line\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         logs\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m: match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_name\n\u001b[1;32m     28\u001b[0m         })\n",
      "File \u001b[0;32m/opt/miniconda3/envs/alertops-ai/lib/python3.9/re.py:191\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged_logs = []\n",
    "\n",
    "for source, file_path in datasets.items():\n",
    "    if file_path.endswith(\".log\"):\n",
    "        print(f\"Parsing log file: {file_path}\")\n",
    "        try:\n",
    "            df = parse_log_file(file_path, source)\n",
    "            merged_logs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "    elif file_path.endswith(\".csv\"):\n",
    "        print(f\"Parsing CSV file: {file_path}\")\n",
    "        try:\n",
    "            df = parse_csv_file(file_path, source)\n",
    "            merged_logs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "\n",
    "final_df = pd.concat(merged_logs, ignore_index=True)\n",
    "\n",
    "final_df.to_csv(\"dataframe_full_view.csv\", index=False)\n",
    "print(f\"Merged logs saved to 'dataframe_full_view.csv'. Total rows: {len(final_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4121576 entries, 0 to 4121575\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   level    object\n",
      " 1   message  object\n",
      " 2   source   object\n",
      "dtypes: object(3)\n",
      "memory usage: 94.3+ MB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alertops-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
