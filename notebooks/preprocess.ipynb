{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis for Loghub Datasets: https://github.com/logpai/loghub?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets variable with paths to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Apache\": \"../datasets/Apache.log\",\n",
    "    \"BGL\": \"../datasets/BGL/BGL.log\",\n",
    "    \"HDFS\": \"../datasets/HDFS_V1/HDFS.log\",\n",
    "    \"Linux\": \"../datasets/Linux.log\",\n",
    "    \"Mac\": \"../datasets/Mac.log\",\n",
    "    \"OpenStack_Normal1\": \"../datasets/OpenStack/openstack_normal1.log\",\n",
    "    \"OpenStack_Normal2\": \"../datasets/OpenStack/openstack_normal2.log\",\n",
    "    \"OpenStack_Abnormal\": \"../datasets/OpenStack/openstack_abnormal.log\",\n",
    "    \"Synthetic_Logs\": \"../datasets/synthetic_logs.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function extracts three parts from the log entry:\n",
    "1. Log level\n",
    "2. Message\n",
    "3. Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(file_path, source_name):\n",
    "    logs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        for i, log_line in enumerate(file):\n",
    "            if i >= 2000000: # Limiting logs to 10000,000\n",
    "                break\n",
    "\n",
    "            match = None\n",
    "            log_pattern = None\n",
    "            try:\n",
    "                if (source_name == \"Apache\"):\n",
    "                    log_pattern = r\"\\[(.*?)\\] \\[(.*?)\\] (.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"BGL\"):\n",
    "                    log_pattern = r\"- \\d+ \\d{4}\\.\\d{2}\\.\\d{2} ([\\w:-]+) [\\d\\-:.]+ [\\w:-]+ ([\\w\\s]+) (.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"HDFS\"):\n",
    "                    log_pattern = r\"\\d{6} \\d{6} \\d+ (INFO|WARN|ERROR) ([\\w\\.$]+): (.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(1),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"Linux\"):\n",
    "                    log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+([\\w-]+)\\s+([\\w.$]+):\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(3),\n",
    "                            \"message\": match.group(4),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"Mac\"):\n",
    "                    log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+([\\w-]+)\\s+([\\w\\[\\]0-9]+):\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(3),\n",
    "                            \"message\": match.group(4),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"OpenStack_Normal1\" or \"OpenStack_Normal2\" or \"OpenStack_Abnormal\"):\n",
    "                    log_pattern = r\"^\\S+\\s+\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\\.\\d+\\s+\\d+\\s+(INFO|ERROR|WARN|DEBUG)\\s+([\\w\\.\\[\\]]+)\\s+(?:\\[.*?\\])?\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"level\": match.group(1),\n",
    "                            \"message\": match.group(2),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "                    \n",
    "    return pd.DataFrame(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_file(file_path, source_name):\n",
    "    logs_df = pd.read_csv(file_path, header=None, names=[\"log_line\"])\n",
    "    log_pattern = r\"^\\[(.*?)\\]\\s+(INFO|WARN|ERROR|CRITICAL)\\s+(\\w+):\\s+(.*)$\"\n",
    "\n",
    "    parsed_logs = []\n",
    "    for log in logs_df[\"log_line\"]:\n",
    "        match = re.match(log_pattern, log)\n",
    "        if match:\n",
    "            parsed_logs.append({\n",
    "                \"level\": match.group(2),      # Log level\n",
    "                \"message\": match.group(4),    # Message\n",
    "                \"source\": match.group(3)     # Source\n",
    "            })\n",
    "    return pd.DataFrame(parsed_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datastets: 100%|██████████| 9/9 [00:33<00:00,  3.74s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed logs saved to 'dataframe_full_view.csv'. Total rows: 5255259\n"
     ]
    }
   ],
   "source": [
    "merged_logs = []\n",
    "\n",
    "with tqdm(total=len(datasets), desc=\"Processing datastets\", unit='file') as pbar:\n",
    "    for source, file_path in datasets.items():\n",
    "        if file_path.endswith(\".log\"):\n",
    "            try:\n",
    "                df = parse_log_file(file_path, source)\n",
    "                merged_logs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file_path}: {e}\")\n",
    "        elif file_path.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = parse_csv_file(file_path, source)\n",
    "                merged_logs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file_path}: {e}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "final_df = pd.concat(merged_logs, ignore_index=True)\n",
    "\n",
    "final_df.to_csv(\"processed_logs.csv\", index=False)\n",
    "print(f\"Processed logs saved to 'dataframe_full_view.csv'. Total rows: {len(final_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alertops-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
