{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing datasets from Loghub Datasets: https://github.com/logpai/loghub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets variable with paths to the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Apache\": \"../datasets/Apache.log\",\n",
    "    \"BGL\": \"../datasets/BGL/BGL.log\",\n",
    "    \"HDFS\": \"../datasets/HDFS_V1/HDFS.log\",\n",
    "    \"Linux\": \"../datasets/Linux.log\",\n",
    "    \"Mac\": \"../datasets/Mac.log\",\n",
    "    \"OpenStack_Normal1\": \"../datasets/OpenStack/openstack_normal1.log\",\n",
    "    \"OpenStack_Normal2\": \"../datasets/OpenStack/openstack_normal2.log\",\n",
    "    \"OpenStack_Abnormal\": \"../datasets/OpenStack/openstack_abnormal.log\",\n",
    "    \"Synthetic_Logs\": \"../datasets/synthetic_logs.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function extracts three parts from the log entry:\n",
    "1. Log level\n",
    "2. Message\n",
    "3. Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(file_path, source_name):\n",
    "    logs = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n",
    "        for i, log_line in enumerate(file):\n",
    "            if i >= 3000000: # Limiting logs to 10000,000\n",
    "                break\n",
    "\n",
    "            match = None\n",
    "            log_pattern = None\n",
    "            try:\n",
    "                if (source_name == \"Apache\"):\n",
    "                    # log_pattern = r\"\\[(.*?)\\] \\[(.*?)\\] (.*)\"\n",
    "                    log_pattern = r\"\\[(.*?)\\]\\s+\\[(.*?)\\]\\s+(.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"timestamp\": match.group(1),\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"BGL\"):\n",
    "                    # log_pattern = r\"- \\d+ \\d{4}\\.\\d{2}\\.\\d{2} ([\\w:-]+) [\\d\\-:.]+ [\\w:-]+ ([\\w\\s]+) (.*)\"\n",
    "                    log_pattern = r\"- \\d+ \\d{4}\\.\\d{2}\\.\\d{2} \\S+ (\\d{4}-\\d{2}-\\d{2}-\\d{2}\\.\\d{2}\\.\\d{2}\\.\\d+) \\S+ RAS KERNEL (\\w+) (.+)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        # Convert detailed timestamp to datetime format\n",
    "                        timestamp_str = match.group(1)\n",
    "                        timestamp = datetime.strptime(timestamp_str, \"%Y-%m-%d-%H.%M.%S.%f\")\n",
    "                        \n",
    "                        logs.append({\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"HDFS\"):\n",
    "                    # log_pattern = r\"\\d{6} \\d{6} \\d+ (INFO|WARN|ERROR) ([\\w\\.$]+): (.*)\"\n",
    "                    log_pattern = r\"(\\d{6})\\s+(\\d{6})\\s+\\d+\\s+(INFO|WARN|ERROR|DEBUG)\\s+([\\w\\.\\$\\*]+):\\s+(.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                         # Extract date and time, then format as datetime\n",
    "                        date_str = match.group(1)  # e.g., 081109\n",
    "                        time_str = match.group(2)  # e.g., 203518\n",
    "                        timestamp = datetime.strptime(date_str + time_str, \"%y%m%d%H%M%S\")\n",
    "                        \n",
    "                        logs.append({\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"level\": match.group(3),\n",
    "                            \"message\": match.group(5),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"Linux\"):\n",
    "                    # log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+([\\w-]+)\\s+([\\w.$]+):\\s+(.*)$\"\n",
    "                    log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d{2}:\\d{2}:\\d{2})\\s+\\S+\\s+([\\w\\-\\.]+):\\s+(.*)$\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"timestamp\": match.group(1),\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"Mac\"):\n",
    "                    # log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d+:\\d+:\\d+)\\s+([\\w-]+)\\s+([\\w\\[\\]0-9]+):\\s+(.*)$\"\n",
    "                    log_pattern = r\"^(\\w{3}\\s+\\d+\\s+\\d{2}:\\d{2}:\\d{2})\\s+\\S+\\s+([\\w\\[\\]]+):\\s+(.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"timestamp\": match.group(1),\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "                elif (source_name == \"OpenStack_Normal1\" or \"OpenStack_Normal2\" or \"OpenStack_Abnormal\"):\n",
    "                    # log_pattern = r\"^\\S+\\s+\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\\.\\d+\\s+\\d+\\s+(INFO|ERROR|WARN|DEBUG)\\s+([\\w\\.\\[\\]]+)\\s+(?:\\[.*?\\])?\\s+(.*)$\"\n",
    "                    log_pattern = r\"^\\S+\\s+(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}\\.\\d+)\\s+\\d+\\s+(INFO|WARN|ERROR|CRITICAL)\\s+\\S+\\s+(.*)\"\n",
    "                    match = re.match(log_pattern, log_line)\n",
    "                    if match is not None:\n",
    "                        logs.append({\n",
    "                            \"timestamp\": match.group(1),\n",
    "                            \"level\": match.group(2),\n",
    "                            \"message\": match.group(3),\n",
    "                            \"source\": source_name\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "                    \n",
    "    return pd.DataFrame(logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_file(file_path, source_name):\n",
    "    logs_df = pd.read_csv(file_path, header=None, names=[\"log_line\"])\n",
    "    log_pattern = r\"^\\[(.*?)\\]\\s+(INFO|WARN|ERROR|CRITICAL)\\s+(\\w+):\\s+(.*)$\"\n",
    "\n",
    "    parsed_logs = []\n",
    "    for log in logs_df[\"log_line\"]:\n",
    "        match = re.match(log_pattern, log)\n",
    "        if match:\n",
    "            parsed_logs.append({\n",
    "                \"level\": match.group(2),      # Log level\n",
    "                \"message\": match.group(4),    # Message\n",
    "                \"source\": match.group(3)     # Source\n",
    "            })\n",
    "    return pd.DataFrame(parsed_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing datastets:  11%|â–ˆ         | 1/9 [00:11<01:35, 11.96s/file]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mparse_log_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         merged_logs\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mparse_log_file\u001b[0;34m(file_path, source_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# Convert detailed timestamp to datetime format\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         timestamp_str \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamp_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH.\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM.\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS.\u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         logs\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: timestamp,\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m\"\u001b[39m: match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     34\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: source_name\n\u001b[1;32m     36\u001b[0m         })\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (source_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDFS\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# log_pattern = r\"\\d{6} \\d{6} \\d+ (INFO|WARN|ERROR) ([\\w\\.$]+): (.*)\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/alertops-ai/lib/python3.9/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    570\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/alertops-ai/lib/python3.9/_strptime.py:422\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# Pad to always return microseconds.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m6\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(s))\n\u001b[0;32m--> 422\u001b[0m     fraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m(s)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m group_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    424\u001b[0m     weekday \u001b[38;5;241m=\u001b[39m locale_time\u001b[38;5;241m.\u001b[39mf_weekday\u001b[38;5;241m.\u001b[39mindex(found_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "merged_logs = []\n",
    "\n",
    "with tqdm(total=len(datasets), desc=\"Processing datastets\", unit='file') as pbar:\n",
    "    for source, file_path in datasets.items():\n",
    "        if file_path.endswith(\".log\"):\n",
    "            try:\n",
    "                df = parse_log_file(file_path, source)\n",
    "                merged_logs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file_path}: {e}\")\n",
    "        elif file_path.endswith(\".csv\"):\n",
    "            try:\n",
    "                df = parse_csv_file(file_path, source)\n",
    "                merged_logs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {file_path}: {e}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "final_df = pd.concat(merged_logs, ignore_index=True)\n",
    "\n",
    "final_df.to_csv(\"processed_logs.csv\", index=False)\n",
    "print(f\"Processed logs saved to 'processed_logs.csv'. Total rows: {len(final_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alertops-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
